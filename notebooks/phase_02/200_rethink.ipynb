{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 200 : Rethink\n",
    "\n",
    "Approaches from phase 1 does not work at all for the new data. We need to rethink our approach.\n",
    "\n",
    "## Web References\n",
    "\n",
    "- [Time Series Forecasting with PyCaret Regression Module](https://towardsdatascience.com/time-series-forecasting-with-pycaret-regression-module-237b703a0c63a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the source training data\n",
    "df_source = pd.read_csv('../../data/input/df_train.csv')\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the competition data\n",
    "df_competition = pd.read_csv('../../data/input/df_test.csv')\n",
    "\n",
    "print(df_source.shape)\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_source.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unused_columns(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove unused columns from the dataset\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "    df_clean.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datatypes(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Set the columns to the correct datatypes\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "    df_clean['time'] = pd.to_datetime(df_clean['time'])\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_valencia_pressure(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Replace the nulls in Valencia_pressure with the mode\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    # forward fill empty values in Valencia_pressure\n",
    "    df_clean['Valencia_pressure'] = df_clean['Valencia_pressure'] \\\n",
    "        .fillna(method='ffill')\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_valencia_wind(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean Valencia wind degrees by striping string & convert to numerical\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    df_clean['Valencia_wind_deg'] = df_clean['Valencia_wind_deg'].str.extract('(\\\\d+)')\n",
    "    df_clean['Valencia_wind_deg'] = pd.to_numeric(df_clean['Valencia_wind_deg'])\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_seville_pressure(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove non-numeric values\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    df_clean['Seville_pressure'] = df_clean['Seville_pressure'] \\\n",
    "        .str.extract('(\\\\d+)') \\\n",
    "        .astype(float)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(dat:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the data\n",
    "    \"\"\"\n",
    "    df_clean = dat.copy()\n",
    "\n",
    "    df_clean = clean_datatypes(df_clean)\n",
    "    df_clean = clean_unused_columns(df_clean)\n",
    "\n",
    "    df_clean = clean_valencia_pressure(df_clean)\n",
    "    df_clean = clean_valencia_wind(df_clean)\n",
    "    df_clean = clean_seville_pressure(df_clean)\n",
    "\n",
    "    return df_clean   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a cleaned dataset to explore\n",
    "df_explore = clean_data(df_source)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains time series data, so we need to investigate if there are any tends and seasonality in the data.\n",
    "\n",
    "We will start by looking at the data summarized (mean) by month to see the big picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the data to daily frequency\n",
    "df_train_resampled = df_explore.resample('M', on='time').mean()\n",
    "\n",
    "# Group the DataFrame by year\n",
    "year_groups = df_train_resampled.groupby(df_train_resampled.index.year)\n",
    "\n",
    "# # Create subplots for each year\n",
    "# fig, axs = plt.subplots(len(year_groups), 1, figsize=(15, 5.5*len(year_groups)))\n",
    "# for i, (year, data) in enumerate(year_groups):\n",
    "#     axs[i].plot(data.index, data['load_shortfall_3h'])\n",
    "#     axs[i].set_xlabel('Time')\n",
    "#     axs[i].set_ylabel('Load Shortfall 3h')\n",
    "#     axs[i].set_title(f'Load Shortfall 3h for {year}')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, (year, data) in enumerate(year_groups):\n",
    "    ax.plot(data.index - pd.DateOffset(years=i), data['load_shortfall_3h'], label=str(year))\n",
    "\n",
    "ax.set_xlabel('Time')\n",
    "ax.set_ylabel('Load Shortfall 3h')\n",
    "ax.set_title('Load Shortfall 3h for each year')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the data is not stationary, there is a clear trend and seasonality.\n",
    "\n",
    "- The **2016** amd **2017** data track each other well and is a good example of showing that there is clear seasonality in the data.\n",
    "\n",
    "\n",
    "- The throughout the year the **2016** data is lower by about the same amount compared to the **2017** data indicating that we might expect a year-on-year increase in the shortfall. This shows that there could be a clear trend in the data.\n",
    "\n",
    "- The **2015** data is following a similar seasonal pattern, but is not tracking the **2016** and **2017** data as well. The trend also does not match with what we see in the other years. We have to be careful when using this data as it might not be representative of the **2018** data we hope to predict. \n",
    "\n",
    "- There could be factors causing the data for **2015** to look different, we could experiment with only using the **2016** and **2017** data for training to see if we can get better results.\n",
    "\n",
    "- Since the data is not stationary with clear trend and seasonality. We will need experiment with removing these to make the data stationary for best results.\n",
    "\n",
    "- We will probably have to be cautious with the sharp drop in the **2017** data in the last month. We can experiment with excluding this month to see if we can get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = clean_datatypes(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_date_features(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Split the date into its separate parts (years, mont, etc.)\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    df_clean['Year'] = df_clean['time'].dt.year\n",
    "    df_clean['Month'] = df_clean['time'].dt.month\n",
    "    df_clean['Day'] = df_clean['time'].dt.day\n",
    "    df_clean['Hour'] = df_clean['time'].dt.hour\n",
    "    df_clean['Day_of_week'] = df_clean['time'].dt.dayofweek\n",
    "    df_clean['Day_of_year'] = df_clean['time'].dt.dayofyear\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_season_feature(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a season feature based on the month.\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    # coding for the seasons\n",
    "    season = {\n",
    "            12:'Winter', 1:'Winter', 2:'Winter',\n",
    "            3:'Spring', 4:'Spring', 5:'Spring',\n",
    "            6:'Summer', 7:'Summer', 8:'Summer',\n",
    "            9:'Autumn', 10:'Autumn', 11:'Autumn'}\n",
    "    \n",
    "    # add a season column based on the mapping\n",
    "    df_clean['Season'] = df_clean.time.dt.month.map(season)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_day_features(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add a feature to indicate if it is a weekday\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "    day_type = []\n",
    "\n",
    "    day_mapping = {\n",
    "        0: 'Weekday', 1: 'Weekday', 2: 'Weekday', 3: 'Weekday',\n",
    "        4: 'Weekday', 5: 'Weekend', 6: 'Weekend'}\n",
    "\n",
    "    # add a day type based on the mapping\n",
    "    df_clean['day_type'] = df_clean.time.dt.day_of_week.map(day_mapping)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_feature(data:pd.DataFrame, column:str, lags:int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add lag features to the dataset\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    for lag in range(1, lags + 1):\n",
    "        df_clean[f'{column}_lag_{lag}'] = df_clean[column].shift(lag)\n",
    "\n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lag_features(data:pd.DataFrame, lags:int) -> pd.DataFrame:\n",
    "    \"\"\"Add lag features to the dataset\"\"\"\n",
    "\n",
    "    df_features = data.copy()\n",
    "    cities = ['Madrid', 'Valencia', 'Seville', 'Bilbao', 'Barcelona']\n",
    "\n",
    "    measures = [\n",
    "        'wind_speed',\n",
    "        'wind_deg',\n",
    "        'rain_1h',\n",
    "        'rain_3h',\n",
    "        'humidity',\n",
    "        'clouds_all',\n",
    "        'pressure',\n",
    "        'snow_3h',\n",
    "        'weather_id',\n",
    "        'temp_max',\n",
    "        'temp_min',\n",
    "        'temp'\n",
    "    ]\n",
    "\n",
    "    for city in cities:\n",
    "        for feature in measures:\n",
    "            if f'{city}_{feature}' in df_features.columns:\n",
    "                df_features = add_lag_feature(df_features, f'{city}_{feature}', lags)\n",
    "            else:\n",
    "                logging.warn(f'{city}_{feature} not in dataset')\n",
    "\n",
    "    return df_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rolling_average_features(data:pd.DataFrame, window:int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add rolling average features to the dataset\n",
    "    \"\"\"\n",
    "    df_features = data.copy()\n",
    "    cities = ['Madrid', 'Valencia', 'Seville', 'Bilbao', 'Barcelona']\n",
    "\n",
    "    measures = [\n",
    "        'wind_speed',\n",
    "        'wind_deg',\n",
    "        'rain_1h',\n",
    "        'rain_3h',\n",
    "        'humidity',\n",
    "        'clouds_all',\n",
    "        'pressure',\n",
    "        'snow_3h',\n",
    "        'weather_id',\n",
    "        'temp_max',\n",
    "        'temp_min',\n",
    "        'temp'\n",
    "    ]\n",
    "\n",
    "    for city in cities:\n",
    "        for feature in measures:\n",
    "            if f'{city}_{feature}' in df_features.columns:\n",
    "                df_features[f'{city}_{feature}_rolling_avg_{window}'] = df_features[f'{city}_{feature}'].rolling(window=window).mean()\n",
    "            else:\n",
    "                logging.warn(f'{city}_{feature} not in dataset')\n",
    "\n",
    "    return df_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data:pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataset and add the features\n",
    "    \"\"\"\n",
    "    df_clean = data.copy()\n",
    "\n",
    "    # define categorical columns\n",
    "    categorical_columns = [\n",
    "        'Month', 'Day', 'Hour', \n",
    "        'Day_of_week', 'Season', 'Day_type'\n",
    "    ]\n",
    "\n",
    "    # perform cleaning and feature engineering\n",
    "    df_clean = clean_data(df_clean)\n",
    "    df_clean = add_date_features(df_clean)\n",
    "    df_clean = add_season_feature(df_clean)\n",
    "    df_clean = add_day_features(df_clean)\n",
    "\n",
    "    # # add rolling average features\n",
    "    # df_clean = add_rolling_average_features(df_clean, 5)\n",
    "    # df_clean = add_rolling_average_features(df_clean, 10)\n",
    "    # df_clean = add_rolling_average_features(df_clean, 30)\n",
    "    # df_clean = add_rolling_average_features(df_clean, 60)\n",
    "    # df_clean = add_rolling_average_features(df_clean, 90)\n",
    "    # df_clean = add_rolling_average_features(df_clean, 365)\n",
    "\n",
    "\n",
    "    # # add lag features\n",
    "    # df_clean = add_lag_features(df_clean, 10)\n",
    "    \n",
    "    # replace empty lag values with 0\n",
    "    df_clean.fillna(0, inplace=True)\n",
    "\n",
    "    return pd.get_dummies(\n",
    "        df_clean, \n",
    "        drop_first=True)\n",
    "    \n",
    "\n",
    "    # return pd.get_dummies(\n",
    "    #     df_clean, \n",
    "    #     columns=categorical_columns,\n",
    "    #     drop_first=True)\n",
    "\n",
    "# clean the dataset and add new features\n",
    "df_features = feature_engineering(df_source)\n",
    "\n",
    "with pd.option_context('display.max_columns', None):\n",
    "    display(df_features.head(3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Trends and Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the data looks like with a moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for plotting\n",
    "df_plot = feature_engineering(df_source)\n",
    "\n",
    "# create a moving average\n",
    "df_plot['moving_average'] = df_plot['load_shortfall_3h'].rolling(8*30).mean()\n",
    "\n",
    "# plot the data and moving average\n",
    "fig = px.line(df_plot, x=\"time\", y=[\"load_shortfall_3h\", \"moving_average\"])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data has so much variation in it that will make prediction very hard. Lets try and zoom in a bit and look at a single month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data for a single month\n",
    "fig = px.line(\n",
    "    df_plot[(df_plot['time'] >= '2016-01-01') & (df_plot['time'] < '2016-02-01')],\n",
    "    x=\"time\", \n",
    "    y=[\"load_shortfall_3h\", \n",
    "    \"moving_average\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get down to day level and see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_days(data:pd.DataFrame, start_date: str, end_date: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot each day in the given date range over the top of each other.\n",
    "    \"\"\"\n",
    "    df = data[(data['time'] >= start_date) & (data['time'] <= end_date)]\n",
    "\n",
    "    # Create a new figure object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Loop over each day and add a line to the plot for that day\n",
    "    for i, day in enumerate(pd.date_range(start=start_date, end=end_date, freq='D')):\n",
    "        # Filter the data for the current day\n",
    "        day_data = df[df['time'].dt.date == day.date()]\n",
    "        \n",
    "        # Plot the data for the current day on the same y-axis\n",
    "        x_values = [t.strftime('%H:%M:%S') for t in day_data['time'].dt.time]\n",
    "        \n",
    "        ax.plot(\n",
    "            x_values, \n",
    "            day_data['load_shortfall_3h'],\n",
    "            label=day.date())\n",
    "\n",
    "    # Set the y-axis label and title\n",
    "    ax.set_ylabel('Load Shortfall 3h')\n",
    "    ax.set_title('Load Shortfall 3h for each day from {} to {}'.format(start_date, end_date))\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('January')\n",
    "plot_days(df_plot, '2015-01-10', '2015-01-15')\n",
    "plot_days(df_plot, '2016-01-10', '2016-01-15')\n",
    "plot_days(df_plot, '2017-01-10', '2017-01-15')\n",
    "\n",
    "print('June')\n",
    "plot_days(df_plot, '2015-06-20', '2015-06-25')\n",
    "plot_days(df_plot, '2016-06-20', '2016-06-25')\n",
    "plot_days(df_plot, '2017-06-20', '2017-06-25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_evaluation(y_test, y_predict):\n",
    "    # calculate the metrics\n",
    "    mae = mean_absolute_error(y_test, y_predict) \n",
    "    mse = mean_squared_error(y_test, y_predict) \n",
    "    rmse = np.sqrt(mse) \n",
    "    r2 = r2_score(y_test, y_predict)\n",
    "\n",
    "    print(f'RMSE: {rmse:.3f} | R-squared: {r2:.4f}')\n",
    "\n",
    "    return rmse, r2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prophet_model(data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create and fit a Prophet model that can be used to predict future\n",
    "    data.\n",
    "    \"\"\"\n",
    "    # perform feature engineering\n",
    "    df_train = feature_engineering(data)\n",
    "\n",
    "    # rename the columns to what prhophet expects\n",
    "    df_train.rename(columns={'load_shortfall_3h': 'y', 'time': 'ds'}, inplace=True)\n",
    "\n",
    "    # create the prophet model\n",
    "    model = Prophet(\n",
    "        seasonality_mode='multiplicative',\n",
    "        changepoint_prior_scale=0.05,\n",
    "        seasonality_prior_scale=0.1,\n",
    "        interval_width=0.8,\n",
    "        yearly_seasonality=True)\n",
    "\n",
    "    # fit the model to the training data\n",
    "    model.fit(df_train)\n",
    "\n",
    "    # return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prophet_predictions(model, data:pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Get predictions from the Prophet model\n",
    "    \"\"\"\n",
    "    df_test = data.copy()\n",
    "\n",
    "    # rename the columns\n",
    "    df_test.rename(columns={'load_shortfall_3h': 'y', 'time': 'ds'}, inplace=True)\n",
    "\n",
    "    # make predictions for the future dates\n",
    "    forecast = model.predict(df_test)\n",
    "\n",
    "    # extract the relevant columns from the forecast\n",
    "    predictions = forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "prophet_model = create_prophet_model(df_source)\n",
    "\n",
    "# get the competition data\n",
    "df_test = feature_engineering(df_competition)\n",
    "\n",
    "# get the predictions\n",
    "predictions = get_prophet_predictions(prophet_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the predictions\n",
    "predictions.plot(x='ds', y='yhat', kind='line', figsize=(15, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data so we can test it with a Kaggel submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_test \\\n",
    "    .rename(columns={'time': 'ds'})[['ds']] \\\n",
    "    .merge(predictions[['ds', 'yhat']], on='ds', how='left') \\\n",
    "    .rename(columns={'ds': 'time'}) \\\n",
    "    .rename(columns={'yhat': 'load_shortfall_3h'})\n",
    "\n",
    "display(df_out)\n",
    "\n",
    "#Output to csv for Kaggle \n",
    "df_out.to_csv(\"../../data/output/200_all_prophet.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a score of`4070.06422` on the public leaderboard. This is not a good good result and puts as at position `74` out of `141` teams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latest Two Years\n",
    "\n",
    "We have seen in our analysis that the **2015** data is potentially not representative of future years, so we will try and build a model without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last two years\n",
    "df_train = df_source[df_features['time'] >= '2016-01-01']\n",
    "\n",
    "# create the model\n",
    "prophet_model = create_prophet_model(df_train)\n",
    "\n",
    "# get the competition data\n",
    "df_test = feature_engineering(df_competition)\n",
    "\n",
    "# get the predictions\n",
    "predictions = get_prophet_predictions(prophet_model, df_test)\n",
    "\n",
    "# plot the predictions\n",
    "predictions.plot(x='ds', y='yhat', kind='line', figsize=(15, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot looks very similar to the plot we saw when we used all the data. This might be an indication that we might not get a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_test \\\n",
    "    .rename(columns={'time': 'ds'})[['ds']] \\\n",
    "    .merge(predictions[['ds', 'yhat']], on='ds', how='left') \\\n",
    "    .rename(columns={'ds': 'time'}) \\\n",
    "    .rename(columns={'yhat': 'load_shortfall_3h'})\n",
    "\n",
    "display(df_out)\n",
    "\n",
    "#Output to csv for Kaggle \n",
    "df_out.to_csv(\"../../data/output/200_last_two_prophet.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excluding **2015** data results in a worse score of `4275.24288` on the public leaderboard."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final try excluding the last month of **2017** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select last two years, but exclude the last month\n",
    "df_train = df_source[\n",
    "    (df_features['time'] >= '2016-01-01') & \\\n",
    "    (df_features['time'] < '2017-12-01')\n",
    "]\n",
    "\n",
    "# create the model\n",
    "prophet_model = create_prophet_model(df_train)\n",
    "\n",
    "# get the competition data\n",
    "df_test = feature_engineering(df_competition)\n",
    "\n",
    "# get the predictions\n",
    "predictions = get_prophet_predictions(prophet_model, df_test)\n",
    "\n",
    "# plot the predictions\n",
    "predictions.plot(x='ds', y='yhat', kind='line', figsize=(15, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_test \\\n",
    "    .rename(columns={'time': 'ds'})[['ds']] \\\n",
    "    .merge(predictions[['ds', 'yhat']], on='ds', how='left') \\\n",
    "    .rename(columns={'ds': 'time'}) \\\n",
    "    .rename(columns={'yhat': 'load_shortfall_3h'})\n",
    "\n",
    "display(df_out)\n",
    "\n",
    "#Output to csv for Kaggle \n",
    "df_out.to_csv(\"../../data/output/200_last_two_sans_12_prophet.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predictions for the last month is higher now. But this has drastically reduced the score to `5135.65675` on the public leaderboard.\n",
    "\n",
    "There is however an interesting idea here that we could try. We could predict the entire **2018** dataset with a baseline model, the use a second model to predict a selected month only and replace this in the base model predicts, then see what influence this has on the Kaggle score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pycaret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see what the data looks like with a moving average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data for plotting\n",
    "df_plot = feature_engineering(df_source)\n",
    "\n",
    "# create a moving average\n",
    "df_plot['moving_average'] = df_plot['load_shortfall_3h'].rolling(8*30).mean()\n",
    "\n",
    "# plot the data and moving average\n",
    "fig = px.line(df_plot, x=\"time\", y=[\"load_shortfall_3h\", \"moving_average\"])\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data has so much variation in it that will make prediction very hard. Lets try and zoom in a bit and look at a single month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data for a single month\n",
    "fig = px.line(\n",
    "    df_plot[(df_plot['time'] >= '2016-01-01') & (df_plot['time'] < '2016-02-01')],\n",
    "    x=\"time\", \n",
    "    y=[\"load_shortfall_3h\", \n",
    "    \"moving_average\"])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get down to day level and see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_days(data:pd.DataFrame, start_date: str, end_date: str) -> None:\n",
    "    \"\"\"\n",
    "    Plot each day in the given date range over the top of each other.\n",
    "    \"\"\"\n",
    "    df = data[(data['time'] >= start_date) & (data['time'] <= end_date)]\n",
    "\n",
    "    # Create a new figure object\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Loop over each day and add a line to the plot for that day\n",
    "    for i, day in enumerate(pd.date_range(start=start_date, end=end_date, freq='D')):\n",
    "        # Filter the data for the current day\n",
    "        day_data = df[df['time'].dt.date == day.date()]\n",
    "        \n",
    "        # Plot the data for the current day on the same y-axis\n",
    "        x_values = [t.strftime('%H:%M:%S') for t in day_data['time'].dt.time]\n",
    "        \n",
    "        ax.plot(\n",
    "            x_values, \n",
    "            day_data['load_shortfall_3h'],\n",
    "            label=day.date())\n",
    "\n",
    "    # Set the y-axis label and title\n",
    "    ax.set_ylabel('Load Shortfall 3h')\n",
    "    ax.set_title('Load Shortfall 3h for each day from {} to {}'.format(start_date, end_date))\n",
    "\n",
    "    # Add a legend\n",
    "    ax.legend()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('January')\n",
    "plot_days(df_plot, '2015-01-10', '2015-01-15')\n",
    "plot_days(df_plot, '2016-01-10', '2016-01-15')\n",
    "plot_days(df_plot, '2017-01-10', '2017-01-15')\n",
    "\n",
    "print('June')\n",
    "plot_days(df_plot, '2015-06-20', '2015-06-25')\n",
    "plot_days(df_plot, '2016-06-20', '2016-06-25')\n",
    "plot_days(df_plot, '2017-06-20', '2017-06-25')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The seasonality is down to the lowest level in the data, namely the 3 hour period.\n",
    "\n",
    "- As a simple experiment we can try and do predictions using the hour as the only feature, but this is what the Prophet is essentially already doing by only using the date as a feature.\n",
    "\n",
    "- We could add a day of year feature and test if it helps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
